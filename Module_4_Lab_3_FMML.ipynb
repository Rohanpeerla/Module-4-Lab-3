{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyJu6UaYkg4CAX6KQ3+ubi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohanpeerla/Module-4-Lab-3/blob/master/Module_4_Lab_3_FMML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Change batch size in mini-batch gradient descent"
      ],
      "metadata": {
        "id": "1z10URodMhMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWkADVl-MgNU",
        "outputId": "0c01c68b-f9d9-4a4b-9c84-3e716b790de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch Size: 10\n",
            "y_pred[0]: 2.009\n",
            "y_pred[1]: 2.860\n",
            "Final error: 5.995\n",
            "\n",
            "Batch Size: 30\n",
            "y_pred[0]: 2.008\n",
            "y_pred[1]: 2.861\n",
            "Final error: 2.261\n",
            "\n",
            "Batch Size: 50\n",
            "y_pred[0]: 2.008\n",
            "y_pred[1]: 2.861\n",
            "Final error: 1.200\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def minibatch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10, batch_size=20):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        indices = np.random.permutation(m)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_i = X[i: i + batch_size]\n",
        "            y_i = y[i: i + batch_size]\n",
        "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "            y_pred = y_pred - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "        cost_history[it] = cost\n",
        "    return y_pred, cost_history\n",
        "def cal_cost(theta, X, y):\n",
        "    m = len(y)\n",
        "    prediction = np.dot(X, theta)\n",
        "    cost = (1 / (2 * m)) * np.sum(np.square(prediction - y))\n",
        "    return cost\n",
        "lr = 0.1\n",
        "n_iter = 200\n",
        "y_pred = np.random.randn(2, 1)\n",
        "X = np.random.randn(100, 1)\n",
        "y = 3 * X + 2 + np.random.randn(100, 1)\n",
        "batch_sizes_to_try = [10, 30, 50]\n",
        "for new_batch_size in batch_sizes_to_try:\n",
        "    y_pred, cost_history = minibatch_gradient_descent(X, y, y_pred, lr, n_iter, batch_size=new_batch_size)\n",
        "    print(f'\\nBatch Size: {new_batch_size}')\n",
        "    print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "    print('Final error: {:0.3f}'.format(cost_history[-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Test all the three out on real datasets."
      ],
      "metadata": {
        "id": "VLlwwNO2NScF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def minibatch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10, batch_size=20):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        indices = np.random.permutation(m)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_i = X[i: i + batch_size]\n",
        "            y_i = y[i: i + batch_size]\n",
        "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "            y_pred = y_pred - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "        cost_history[it] = cost\n",
        "    return y_pred, cost_history\n",
        "def cal_cost(theta, X, y):\n",
        "    m = len(y)\n",
        "    prediction = np.dot(X, theta)\n",
        "    cost = (1 / (2 * m)) * np.sum(np.square(prediction - y))\n",
        "    return cost\n",
        "lr = 0.01\n",
        "n_iter = 1000\n",
        "y_pred = np.random.randn(2, 1)\n",
        "X_real = np.random.randn(100, 1)\n",
        "y_real = 3 * X_real + 2 + np.random.randn(100, 1)\n",
        "batch_sizes_to_try = [10, 30, 50]\n",
        "for new_batch_size in batch_sizes_to_try:\n",
        "    y_pred, cost_history = minibatch_gradient_descent(X_real, y_real, y_pred, lr, n_iter, batch_size=new_batch_size)\n",
        "    print(f'\\nBatch Size: {new_batch_size}')\n",
        "    print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "    print('Final error: {:0.3f}'.format(cost_history[-1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC58nEAFNS4e",
        "outputId": "3eeda02c-1088-4f9b-97ae-6ab19bf63191"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch Size: 10\n",
            "y_pred[0]: 1.973\n",
            "y_pred[1]: 3.039\n",
            "Final error: 4.783\n",
            "\n",
            "Batch Size: 30\n",
            "y_pred[0]: 1.973\n",
            "y_pred[1]: 3.040\n",
            "Final error: 1.814\n",
            "\n",
            "Batch Size: 50\n",
            "y_pred[0]: 1.973\n",
            "y_pred[1]: 3.040\n",
            "Final error: 0.957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Compare the effects of changing learning rate by the same amount in Batch GD, SGD and Mini-batch GD."
      ],
      "metadata": {
        "id": "_li-O0d1NmF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def batch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for it in range(iterations):\n",
        "        prediction = np.dot(X, y_pred)\n",
        "        y_pred = y_pred - (1 / m) * learning_rate * (X.T.dot((prediction - y)))\n",
        "        cost_history[it] = cal_cost(y_pred, X, y)\n",
        "    return y_pred, cost_history\n",
        "def stochastic_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for it in range(iterations):\n",
        "        for i in range(m):\n",
        "            X_i = np.c_[1, X[i]]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "            y_pred = y_pred - learning_rate * (X_i.T.dot((prediction - y[i])))\n",
        "            cost_history[it] += cal_cost(y_pred, X_i, y[i])\n",
        "    return y_pred, cost_history\n",
        "def minibatch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10, batch_size=20):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        indices = np.random.permutation(m)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_i = X[i: i + batch_size]\n",
        "            y_i = y[i: i + batch_size]\n",
        "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "            y_pred = y_pred - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "        cost_history[it] = cost\n",
        "    return y_pred, cost_history\n",
        "def cal_cost(theta, X, y):\n",
        "    m = len(y)\n",
        "    prediction = np.dot(X, theta)\n",
        "    cost = (1 / (2 * m)) * np.sum(np.square(prediction - y))\n",
        "    return cost\n",
        "n_iter = 1000\n",
        "y_pred_batch = np.random.randn(2, 1)\n",
        "y_pred_stochastic = np.random.randn(2, 1)\n",
        "y_pred_minibatch = np.random.randn(2, 1)\n",
        "X_real = np.random.randn(100, 1)\n",
        "y_real = 3 * X_real + 2 + np.random.randn(100, 1)\n",
        "learning_rates_to_try = [0.01, 0.1, 0.5]\n",
        "for lr in learning_rates_to_try:\n",
        "    y_pred_batch, cost_history_batch = batch_gradient_descent(np.c_[np.ones_like(X_real), X_real], y_real, y_pred_batch, lr, n_iter)\n",
        "    y_pred_stochastic, cost_history_stochastic = stochastic_gradient_descent(X_real, y_real, y_pred_stochastic, lr, n_iter)\n",
        "    y_pred_minibatch, cost_history_minibatch = minibatch_gradient_descent(X_real, y_real, y_pred_minibatch, lr, n_iter, batch_size=20)\n",
        "    print(f'\\nLearning Rate: {lr}')\n",
        "    print('Batch GD - Final error: {:0.3f}'.format(cost_history_batch[-1]))\n",
        "    print('SGD - Final error: {:0.3f}'.format(cost_history_stochastic[-1]))\n",
        "    print('Mini-batch GD - Final error: {:0.3f}'.format(cost_history_minibatch[-1]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omlcUHgGNruR",
        "outputId": "aa64b95c-7ee1-45d8-e152-f461904e044e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learning Rate: 0.01\n",
            "Batch GD - Final error: 0.640\n",
            "SGD - Final error: 62.700\n",
            "Mini-batch GD - Final error: 3.199\n",
            "\n",
            "Learning Rate: 0.1\n",
            "Batch GD - Final error: 0.640\n",
            "SGD - Final error: 48.440\n",
            "Mini-batch GD - Final error: 3.191\n",
            "\n",
            "Learning Rate: 0.5\n",
            "Batch GD - Final error: 0.640\n",
            "SGD - Final error: 28.427\n",
            "Mini-batch GD - Final error: 3.187\n"
          ]
        }
      ]
    }
  ]
}